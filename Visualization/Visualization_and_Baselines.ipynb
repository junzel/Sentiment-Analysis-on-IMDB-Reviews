{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: <class 'list'>\n",
      "Samples per class (training): [12500 12500]\n",
      "Number of reviews: <class 'list'>\n",
      "Samples per class (training): [12500 12500]\n",
      "Loading Completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pdb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "TRAIN_PATH = '../aclImdb/train/'\n",
    "TEST_PATH = '../aclImdb/test/'\n",
    "\n",
    "# each data sample (review) is a document of words\n",
    "# each review is either negative (0) or positive (1)\n",
    "reviews_train = load_files(TRAIN_PATH)\n",
    "data_train, targets_train = reviews_train.data, reviews_train.target\n",
    "print(\"Number of reviews: {}\".format(type(data_train))) # 25000\n",
    "print(\"Samples per class (training): {}\".format(np.bincount(targets_train)))\n",
    "\n",
    "reviews_test = load_files(TEST_PATH)\n",
    "data_test, targets_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of reviews: {}\".format(type(data_train))) # 25000\n",
    "print(\"Samples per class (training): {}\".format(np.bincount(targets_train)))\n",
    "\n",
    "# remove line breakers\n",
    "data_train = [review.replace(b\"<br />\", b\" \") for review in data_train]\n",
    "data_test = [review.replace(b\"<br />\", b\" \") for review in data_test]\n",
    "\n",
    "print(\"Loading Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 74849\n",
      "Features 20010 to 20030:\n",
      "['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "(25000, 74849) (25000, 74849)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(data_train)\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Size of vocabulary: {}\".format(len(feature_names)))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "\n",
    "X_train = vect.transform(data_train) # 25000 x 74849\n",
    "X_test = vect.transform(data_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Validation Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 74849) (5000, 74849)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_real, X_val, targets_train_real, targets_val = train_test_split(X_train, targets_train, test_size=0.2, random_state=42)\n",
    "print(X_train_real.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (Logistic regresssion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# scores = cross_val_score(LogisticRegression(solver='lbfgs',max_iter=200), X_train, targets_train, cv=5)\n",
    "# print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))\n",
    "clf = LogisticRegression(random_state=0).fit(X_train_real, targets_train_real)\n",
    "pred = clf.predict(X_test)\n",
    "# print((targets_test == pred)\n",
    "print((pred == targets_test).sum() / len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10, 10, 20, 20, 10), random_state=1)\n",
    "clf.fit(X_train_real, targets_train_real)\n",
    "pred = clf.predict(X_test)\n",
    "print((pred == targets_test).sum() / len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8666\n",
      "Test accuracy: 0.84684\n"
     ]
    }
   ],
   "source": [
    "arch_list = [[], [], [], [], [] ,[]]\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(8,8,16,16,8,8), activation='identity', random_state=1)\n",
    "clf.fit(X_train_real, targets_train_real)\n",
    "pred = clf.predict(X_val)\n",
    "print(\"Validation accuracy:\", (pred == targets_val).sum() / len(pred))\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Test accuracy:\", (pred == targets_test).sum() / len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8867670410843221\n",
      "(20000, 1000)\n",
      "0.9272125141210419\n",
      "(5000, 1000)\n",
      "0.8876231102154288\n",
      "(25000, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=7, random_state=42)\n",
    "svd.fit(X_train_real)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "X_train_PCA = svd.transform(X_train_real)\n",
    "print(X_train_PCA.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=7, random_state=42)\n",
    "svd.fit(X_val)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "X_val_PCA = svd.transform(X_val)\n",
    "print(X_val_PCA.shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, n_iter=7, random_state=42)\n",
    "svd.fit(X_test)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "X_test_PCA = svd.transform(X_test)\n",
    "print(X_test_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.randint(1, 5, size=2)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i, label in enumerate([0, 1]):\n",
    "    print(label)\n",
    "    plt.scatter(X_train_PCA[targets_train_real == label, 0], X_train_PCA[targets_train_real == label, 1], label=str(label, 'utf-8'), cmap=\"Spectral\", marker='.', alpha=0.5)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--')\n",
    "plt.title('PCA of Pancancer dataset')\n",
    "plt.show();\n",
    "# fig.savefig(\"pca_larger.png\", dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'whelan', u'whelming', u'whereever', u'whetted', u'whic', u'whick', u'whidbey', u'whig', u'whigham', u'whiile']\n",
      "[u'anbuselvan', u'anchorpoint', u'anch\\xeda', u'ancona', u'anda', u'andaaz', u'andalusia', u'anddd', u'andelou', u'andersen']\n"
     ]
    }
   ],
   "source": [
    "vect_fit = vect.fit_transform(data_train)\n",
    "\n",
    "neg_include = targets_train == 0\n",
    "pos_include = targets_train == 1\n",
    "neg_word_freq = np.array(vect_fit[neg_include].sum(axis=0))[0]\n",
    "pos_word_freq = np.array(vect_fit[pos_include].sum(axis=0))[0]\n",
    "assert neg_include.sum() + pos_include.sum() == len(targets_train)\n",
    "\n",
    "neg_sorted_names = [name for _, name in sorted(zip(neg_word_freq, feature_names))]\n",
    "pos_sorted_names = [name for _, name in sorted(zip(pos_word_freq, feature_names))]\n",
    "neg_sorted_freq = sorted(neg_word_freq)\n",
    "pos_sorted_freq = sorted(pos_word_freq)\n",
    "\n",
    "# Some wrongly spelled words in reviews\n",
    "print(neg_sorted_names[20000:20010])\n",
    "print(pos_sorted_names[20000:20010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot\n",
    "# print(list(range(len(neg_sorted_freq[0:50]))))\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].bar(list(range(len(neg_sorted_freq[0:10]))), neg_sorted_freq[0:10])\n",
    "# axs[0].set_xticks(list(range(len(neg_sorted_freq[0:50]))), neg_sorted_names[0:50])\n",
    "# axs[1].bar(list(range(len(pos_sorted_freq[0:50]))), pos_sorted_freq[0:50])\n",
    "# axs[1].set_xticks(list(range(len(pos_sorted_freq[0:50]))), pos_sorted_names[0:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
